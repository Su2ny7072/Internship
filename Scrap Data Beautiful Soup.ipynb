{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec85664",
   "metadata": {},
   "source": [
    "1)Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(search_query):\n",
    "    base_url = f'https://www.amazon.in/s?k={search_query.replace(\" \", \"+\")}'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find and print product titles\n",
    "        product_titles = soup.find_all('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "        for index, title in enumerate(product_titles, start=1):\n",
    "            print(f'{index}. {title.text.strip()}')\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_query = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "    search_amazon_products(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa23406",
   "metadata": {},
   "source": [
    "2)In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74617180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape Amazon product details\n",
    "def scrape_amazon_products(search_query, max_pages=3):\n",
    "    base_url = f'https://www.amazon.in/s?k={search_query.replace(\" \", \"+\")}'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    product_data = {\n",
    "        \"Brand Name\": [],\n",
    "        \"Name of the Product\": [],\n",
    "        \"Price\": [],\n",
    "        \"Return/Exchange\": [],\n",
    "        \"Expected Delivery\": [],\n",
    "        \"Availability\": [],\n",
    "        \"Product URL\": [],\n",
    "    }\n",
    "\n",
    "    page_num = 1\n",
    "    while page_num <= max_pages:\n",
    "        url = f'{base_url}&page={page_num}'\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', {'data-asin': True})\n",
    "        if not products:\n",
    "            break\n",
    "\n",
    "        for product in products:\n",
    "            product_url = f\"https://www.amazon.in{product.find('a', {'class': 'a-link-normal'})['href']}\"\n",
    "            brand_name = product.find('span', {'class': 'a-size-base-plus'}).text.strip()\n",
    "            product_name = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
    "            price = product.find('span', {'class': 'a-price'}).text.strip()\n",
    "            return_exchange = product.find('div', {'class': 'a-row a-size-base a-color-secondary'}).text.strip()\n",
    "            expected_delivery = product.find('div', {'class': 'a-row s-align-children-center'}).text.strip()\n",
    "            availability = product.find('div', {'class': 'a-row a-size-base a-color-secondary'}).find_next('div').text.strip()\n",
    "\n",
    "            product_data[\"Brand Name\"].append(brand_name)\n",
    "            product_data[\"Name of the Product\"].append(product_name)\n",
    "            product_data[\"Price\"].append(price)\n",
    "            product_data[\"Return/Exchange\"].append(return_exchange)\n",
    "            product_data[\"Expected Delivery\"].append(expected_delivery)\n",
    "            product_data[\"Availability\"].append(availability)\n",
    "            product_data[\"Product URL\"].append(product_url)\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "    return pd.DataFrame(product_data)\n",
    "\n",
    "# Input from the user\n",
    "search_query = input(\"Enter the product you want to search on Amazon: \")\n",
    "max_pages = int(input(\"Enter the maximum number of pages to scrape: \"))\n",
    "\n",
    "# Scrape and save data to a CSV\n",
    "data_frame = scrape_amazon_products(search_query, max_pages)\n",
    "data_frame.to_csv(f'{search_query}_products.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91fd98",
   "metadata": {},
   "source": [
    "3)Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Function to scrape images from Google Images\n",
    "def scrape_google_images(search_keywords, num_images=10):\n",
    "    driver = webdriver.Chrome(executable_path=\"path/to/chromedriver\")\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "    driver.maximize_window()\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    for keyword in search_keywords:\n",
    "        search_bar = driver.find_element(By.NAME, \"q\")\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(keyword)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll down to load more images\n",
    "        for _ in range(3):  # Scroll down 3 times\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Get image URLs\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        img_tags = soup.find_all('img', class_='rg_i')\n",
    "\n",
    "        for img in img_tags[:num_images]:\n",
    "            if img.get('data-src'):\n",
    "                image_urls.append(img.get('data-src'))\n",
    "\n",
    "    driver.quit()\n",
    "    return image_urls\n",
    "\n",
    "# Function to download and save images\n",
    "def download_and_save_images(image_urls, save_path='./images/'):\n",
    "    for i, url in enumerate(image_urls):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(f'{save_path}image_{i+1}.jpg', 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images_per_keyword = 10\n",
    "\n",
    "    image_urls = scrape_google_images(search_keywords, num_images_per_keyword)\n",
    "    download_and_save_images(image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1738f",
   "metadata": {},
   "source": [
    "4)Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape smartphone details from Flipkart\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = f'https://www.flipkart.com/search?q={search_query.replace(\" \", \"+\")}'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    product_data = {\n",
    "        \"Brand Name\": [],\n",
    "        \"Smartphone Name\": [],\n",
    "        \"Colour\": [],\n",
    "        \"RAM\": [],\n",
    "        \"Storage(ROM)\": [],\n",
    "        \"Primary Camera\": [],\n",
    "        \"Secondary Camera\": [],\n",
    "        \"Display Size\": [],\n",
    "        \"Battery Capacity\": [],\n",
    "        \"Price\": [],\n",
    "        \"Product URL\": [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        for product in products:\n",
    "            brand_name = product.find('div', {'class': '_4rR01T'}).text.strip()\n",
    "            smartphone_name = product.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "            color = product.find('div', {'class': '_1I3t12'}).text.strip()\n",
    "            specs = product.find_all('li', {'class': 'rgWa7D'})\n",
    "            ram, storage, primary_camera, secondary_camera, display_size, battery_capacity = [\"-\"] * 6\n",
    "            for spec in specs:\n",
    "                text = spec.text.strip()\n",
    "                if \"RAM\" in text:\n",
    "                    ram = text\n",
    "                elif \"ROM\" in text:\n",
    "                    storage = text\n",
    "                elif \"Primary Camera\" in text:\n",
    "                    primary_camera = text\n",
    "                elif \"Secondary Camera\" in text:\n",
    "                    secondary_camera = text\n",
    "                elif \"Display Size\" in text:\n",
    "                    display_size = text\n",
    "                elif \"Battery Capacity\" in text:\n",
    "                    battery_capacity = text\n",
    "            price = product.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "            product_url = f\"https://www.flipkart.com{product.find('a', {'class': 'IRpwTa'})['href']}\"\n",
    "\n",
    "            product_data[\"Brand Name\"].append(brand_name)\n",
    "            product_data[\"Smartphone Name\"].append(smartphone_name)\n",
    "            product_data[\"Colour\"].append(color)\n",
    "            product_data[\"RAM\"].append(ram)\n",
    "            product_data[\"Storage(ROM)\"].append(storage)\n",
    "            product_data[\"Primary Camera\"].append(primary_camera)\n",
    "            product_data[\"Secondary Camera\"].append(secondary_camera)\n",
    "            product_data[\"Display Size\"].append(display_size)\n",
    "            product_data[\"Battery Capacity\"].append(battery_capacity)\n",
    "            product_data[\"Price\"].append(price)\n",
    "            product_data[\"Product URL\"].append(product_url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return pd.DataFrame(product_data)\n",
    "\n",
    "if __name__ == '__main':\n",
    "    search_query = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "    data_frame = scrape_flipkart_smartphones(search_query)\n",
    "    data_frame.to_csv(f'{search_query}_smartphones.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8893ad1",
   "metadata": {},
   "source": [
    "5)Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    api_key = 'https://developers.google.com/maps/gmp-get-started'  # Replace with your Google Maps API key\n",
    "    geolocator = GoogleV3(api_key=api_key)\n",
    "    \n",
    "    try:\n",
    "        location = geolocator.geocode(city_name)\n",
    "        if location:\n",
    "            latitude, longitude = location.latitude, location.longitude\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "    coordinates = get_coordinates(city_name)\n",
    "    \n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {city_name}: Latitude {coordinates[0]}, Longitude {coordinates[1]}\")\n",
    "    else:\n",
    "        print(f\"Coordinates for {city_name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c0a5e",
   "metadata": {},
   "source": [
    "6)Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51838808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        laptops_data = []\n",
    "\n",
    "        product_cards = soup.find_all('div', class_='TopNumbeHeading')\n",
    "        for card in product_cards:\n",
    "            laptop_data = {}\n",
    "\n",
    "            title = card.find('div', class_='TopNumbeHeading').text\n",
    "            specs = card.find('div', class_='Specs').text\n",
    "            price = card.find('div', class_='Price').text\n",
    "            ratings = card.find('div', class_='starRating').text\n",
    "\n",
    "            laptop_data['Title'] = title\n",
    "            laptop_data['Specs'] = specs\n",
    "            laptop_data['Price'] = price\n",
    "            laptop_data['Ratings'] = ratings\n",
    "\n",
    "            laptops_data.append(laptop_data)\n",
    "\n",
    "        return laptops_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main':\n",
    "    gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops:\n",
    "        for index, laptop in enumerate(gaming_laptops, start=1):\n",
    "            print(f\"Laptop {index}:\")\n",
    "            print(f\"Title: {laptop['Title']}\")\n",
    "            print(f\"Specs: {laptop['Specs']}\")\n",
    "            print(f\"Price: {laptop['Price']}\")\n",
    "            print(f\"Ratings: {laptop['Ratings']}\")\n",
    "            print()\n",
    "\n",
    "        # You can save the data to a CSV file if needed\n",
    "        # Example: pd.DataFrame(gaming_laptops).to_csv('gaming_laptops.csv', index=False)\n",
    "    else:\n",
    "        print(\"No gaming laptops data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f5176",
   "metadata": {},
   "source": [
    "7)Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        billionaires_data = []\n",
    "\n",
    "        rows = soup.find_all('div', class_='rank')\n",
    "        for row in rows:\n",
    "            billionaire_data = {}\n",
    "            \n",
    "            rank = row.find('span', class_='num').text.strip()\n",
    "            name = row.find('h2', class_='title').text.strip()\n",
    "            net_worth = row.find('div', class_='netWorth').text.strip()\n",
    "            age = row.find('div', class_='age').text.strip()\n",
    "            citizenship = row.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = row.find('div', class_='source').text.strip()\n",
    "            industry = row.find('div', class_='category').text.strip()\n",
    "\n",
    "            billionaire_data['Rank'] = rank\n",
    "            billionaire_data['Name'] = name\n",
    "            billionaire_data['Net worth'] = net_worth\n",
    "            billionaire_data['Age'] = age\n",
    "            billionaire_data['Citizenship'] = citizenship\n",
    "            billionaire_data['Source'] = source\n",
    "            billionaire_data['Industry'] = industry\n",
    "\n",
    "            billionaires_data.append(billionaire_data)\n",
    "\n",
    "        return billionaires_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main':\n",
    "    billionaires = scrape_forbes_billionaires()\n",
    "\n",
    "    if billionaires:\n",
    "        for billionaire in billionaires:\n",
    "            print(\"Rank:\", billionaire['Rank'])\n",
    "            print(\"Name:\", billionaire['Name'])\n",
    "            print(\"Net worth:\", billionaire['Net worth'])\n",
    "            print(\"Age:\", billionaire['Age'])\n",
    "            print(\"Citizenship:\", billionaire['Citizenship'])\n",
    "            print(\"Source:\", billionaire['Source'])\n",
    "            print(\"Industry:\", billionaire['Industry'])\n",
    "            print()\n",
    "\n",
    "        # You can save the data to a CSV file if needed\n",
    "        # Example: pd.DataFrame(billionaires).to_csv('forbes_billionaires.csv', index=False)\n",
    "    else:\n",
    "        print(\"No billionaire data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85fef3",
   "metadata": {},
   "source": [
    "8)Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10311a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in c:\\anaconda\\lib\\site-packages (2.107.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in c:\\anaconda\\lib\\site-packages (from google-api-python-client) (0.1.1)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in c:\\anaconda\\lib\\site-packages (from google-api-python-client) (2.23.4)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in c:\\anaconda\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\anaconda\\lib\\site-packages (from google-api-python-client) (2.14.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\anaconda\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\anaconda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda\\lib\\site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\anaconda\\lib\\site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda\\lib\\site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\anaconda\\lib\\site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n"
     ]
    }
   ],
   "source": [
    "'google-api-python-client'\n",
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f69599c",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=VIDEO_ID&textFormat=plainText&maxResults=100&key=YOUR_YOUTUBE_API_KEY&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m comments\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     comments \u001b[38;5;241m=\u001b[39m \u001b[43mextract_youtube_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVIDEO_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m comments:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, comment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(comments[:\u001b[38;5;241m500\u001b[39m], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mextract_youtube_comments\u001b[1;34m(api_key, video_url)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mcommentThreads()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m     20\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m         videoId\u001b[38;5;241m=\u001b[39mvideo_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     29\u001b[0m         comment_text \u001b[38;5;241m=\u001b[39m comment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopLevelComment\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextDisplay\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=VIDEO_ID&textFormat=plainText&maxResults=100&key=YOUR_YOUTUBE_API_KEY&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = 'YOUR_YOUTUBE_API_KEY'\n",
    "\n",
    "# YouTube video URL for the video you want to extract comments from\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=VIDEO_ID'\n",
    "\n",
    "# Function to extract comments, comment upvotes, and time from a YouTube video\n",
    "def extract_youtube_comments(api_key, video_url):\n",
    "    video_id = video_url.split(\"v=\")[1]\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for comment in response[\"items\"]:\n",
    "            comment_text = comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comment_upvotes = comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "            comment_time = comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            comments.append({\n",
    "                \"Comment\": comment_text,\n",
    "                \"Upvotes\": comment_upvotes,\n",
    "                \"Time\": comment_time\n",
    "            })\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            next_page_token = response[\"nextPageToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    comments = extract_youtube_comments(API_KEY, VIDEO_URL)\n",
    "\n",
    "    if comments:\n",
    "        for index, comment in enumerate(comments[:500], start=1):\n",
    "            print(f\"Comment {index}:\")\n",
    "            print(f\"Comment: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "            print(f\"Time: {comment['Time']}\")\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810becb",
   "metadata": {},
   "source": [
    "9)Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fb2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostelworld_data():\n",
    "    url = 'https://www.hostelworld.com/s?q=London&country=England'\n",
    "    headers = {\n",
    "        'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        hostels_data = []\n",
    "\n",
    "        hostel_cards = soup.find_all('div', class_='property-card')\n",
    "        for card in hostel_cards:\n",
    "            hostel_data = {}\n",
    "            \n",
    "            hostel_name = card.find('h2', class_='title-2').text.strip()\n",
    "            distance_from_city_center = card.find('span', class_='reviews').find('span', {'class': 'label'}).text.strip()\n",
    "            ratings = card.find('div', class_='score orange').text.strip()\n",
    "            total_reviews = card.find('span', {'class': 'reviews'}).text.strip()\n",
    "            overall_reviews = card.find('div', {'class': 'keyword'}).text.strip()\n",
    "            privates_from_price = card.find('span', class_='price').text.strip()\n",
    "            dorms_from_price = card.find('div', {'class': 'rate'}).find('div').text.strip()\n",
    "            facilities = ', '.join([fac.text.strip() for fac in card.find_all('li', {'class': 'facility-badge'})])\n",
    "            property_description = card.find('p', {'class': 'description'}).text.strip()\n",
    "\n",
    "            hostel_data['Hostel Name'] = hostel_name\n",
    "            hostel_data['Distance from City Center'] = distance_from_city_center\n",
    "            hostel_data['Ratings'] = ratings\n",
    "            hostel_data['Total Reviews'] = total_reviews\n",
    "            hostel_data['Overall Reviews'] = overall_reviews\n",
    "            hostel_data['Privates from Price'] = privates_from_price\n",
    "            hostel_data['Dorms from Price'] = dorms_from_price\n",
    "            hostel_data['Facilities'] = facilities\n",
    "            hostel_data['Property Description'] = property_description\n",
    "\n",
    "            hostels_data.append(hostel_data)\n",
    "\n",
    "        return hostels_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main':\n",
    "    hostels = scrape_hostelworld_data()\n",
    "\n",
    "    if hostels:\n",
    "        for index, hostel in enumerate(hostels, start=1):\n",
    "            print(f\"Hostel {index}:\")\n",
    "            print(f\"Hostel Name: {hostel['Hostel Name']}\")\n",
    "            print(f\"Distance from City Center: {hostel['Distance from City Center']}\")\n",
    "            print(f\"Ratings: {hostel['Ratings']}\")\n",
    "            print(f\"Total Reviews: {hostel['Total Reviews']}\")\n",
    "            print(f\"Overall Reviews: {hostel['Overall Reviews']}\")\n",
    "            print(f\"Privates from Price: {hostel['Privates from Price']}\")\n",
    "            print(f\"Dorms from Price: {hostel['Dorms from Price']}\")\n",
    "            print(f\"Facilities: {hostel['Facilities']}\")\n",
    "            print(f\"Property Description: {hostel['Property Description']}\")\n",
    "            print()\n",
    "\n",
    "        # You can save the data to a CSV file if needed\n",
    "        # Example: pd.DataFrame(hostels).to_csv('hostelworld_data.csv', index=False)\n",
    "    else:\n",
    "        print(\"No hostel data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a0a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
