{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02be27d9",
   "metadata": {},
   "source": [
    "# Web Scraping On Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d79af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\anaconda\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\anaconda\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854eab02",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12e9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0054ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Header Text\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all header tags (h1, h2, h3, h4, h5, h6)\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Extract header text\n",
    "header_text = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Header Text': header_text})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148041c",
   "metadata": {},
   "source": [
    "2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a6b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\anaconda\\lib\\site-packages (4.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a5329d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "# Send a GET request to the website\n",
    "url = 'https://presidentofindia.nic.in/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using lxml\n",
    "tree = html.fromstring(response.content)\n",
    "\n",
    "# Extract data using XPath\n",
    "names = tree.xpath('//td[@class=\"style_4\"]/text()')\n",
    "terms = tree.xpath('//td[@class=\"style_5\"]/text()')\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f20d8a",
   "metadata": {},
   "source": [
    "3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fdf1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team Matches Points                                             Rating\n",
      "0    1      49  5,839  \\n                            119\\n           ...\n",
      "1    2      36  4,015                                                112\n",
      "2    3      32  3,525                                                110\n",
      "3    4      29  3,166                                                109\n",
      "4    5      38  4,007                                                105\n",
      "5    6      34  3,377                                                 99\n",
      "6    7      43  3,943                                                 92\n",
      "7    8      40  3,574                                                 89\n",
      "8    9      26  2,170                                                 83\n",
      "9   10      38  2,582                                                 68\n",
      "                                              Player  \\\n",
      "0  \\n\\n\\n                            1\\n         ...   \n",
      "1  \\n\\n\\n                                    2\\n ...   \n",
      "2  \\n\\n\\n                                    3\\n ...   \n",
      "3  \\n\\n\\n                                    4\\n ...   \n",
      "4  \\n\\n\\n                                    5\\n ...   \n",
      "5  \\n\\n\\n                                    =\\n ...   \n",
      "6  \\n\\n\\n                                    7\\n ...   \n",
      "7  \\n\\n\\n                                    8\\n ...   \n",
      "8  \\n\\n\\n                                    9\\n ...   \n",
      "9  \\n\\n\\n                                    10\\n...   \n",
      "\n",
      "                               Team         Rating  \n",
      "0  \\n\\n\\n\\n\\n\\n\\n\\nBabar Azam\\n\\n\\n  \\n\\n\\nPAK\\n\\n  \n",
      "1                  \\nShubman Gill\\n      \\n\\nIND\\n  \n",
      "2               \\nQuinton de Kock\\n       \\n\\nSA\\n  \n",
      "3              \\nHeinrich Klaasen\\n       \\n\\nSA\\n  \n",
      "4                  \\nDavid Warner\\n      \\n\\nAUS\\n  \n",
      "5                   \\nVirat Kohli\\n      \\n\\nIND\\n  \n",
      "6                  \\nHarry Tector\\n      \\n\\nIRE\\n  \n",
      "7                  \\nRohit Sharma\\n      \\n\\nIND\\n  \n",
      "8         \\nRassie van der Dussen\\n       \\n\\nSA\\n  \n",
      "9                   \\nImam-ul-Haq\\n      \\n\\nPAK\\n  \n",
      "                                              Player  \\\n",
      "0  \\n\\n\\n                            1\\n         ...   \n",
      "1  \\n\\n\\n                                    2\\n ...   \n",
      "2  \\n\\n\\n                                    3\\n ...   \n",
      "3  \\n\\n\\n                                    4\\n ...   \n",
      "4  \\n\\n\\n                                    5\\n ...   \n",
      "5  \\n\\n\\n                                    6\\n ...   \n",
      "6  \\n\\n\\n                                    7\\n ...   \n",
      "7  \\n\\n\\n                                    8\\n ...   \n",
      "8  \\n\\n\\n                                    9\\n ...   \n",
      "9  \\n\\n\\n                                    10\\n...   \n",
      "\n",
      "                                   Team         Rating  \n",
      "0  \\n\\n\\n\\n\\n\\n\\n\\nJosh Hazlewood\\n\\n\\n  \\n\\n\\nAUS\\n\\n  \n",
      "1                    \\nMohammed Siraj\\n      \\n\\nIND\\n  \n",
      "2                    \\nKeshav Maharaj\\n       \\n\\nSA\\n  \n",
      "3                       \\nRashid Khan\\n      \\n\\nAFG\\n  \n",
      "4                       \\nTrent Boult\\n       \\n\\nNZ\\n  \n",
      "5                     \\nMohammad Nabi\\n      \\n\\nAFG\\n  \n",
      "6                        \\nAdam Zampa\\n      \\n\\nAUS\\n  \n",
      "7                        \\nMatt Henry\\n       \\n\\nNZ\\n  \n",
      "8                     \\nKuldeep Yadav\\n      \\n\\nIND\\n  \n",
      "9                    \\nShaheen Afridi\\n      \\n\\nPAK\\n  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape ODI Team Rankings\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "teams = []\n",
    "matches = [] \n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "for row in soup.find('table', class_='table').find_all('tr')[1:11]:\n",
    "    data = row.find_all('td')\n",
    "    teams.append(data[0].text)\n",
    "    matches.append(data[2].text)\n",
    "    points.append(data[3].text)\n",
    "    ratings.append(data[4].text)\n",
    "    \n",
    "teams_df = pd.DataFrame({\n",
    "    'Team': teams,\n",
    "    'Matches': matches, \n",
    "    'Points': points,\n",
    "    'Rating': ratings\n",
    "})\n",
    "\n",
    "# Scrape ODI Batsmen Rankings \n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for row in soup.find('table', class_='table').find_all('tr')[1:11]:\n",
    "    data = row.find_all('td')\n",
    "    players.append(data[0].text)\n",
    "    teams.append(data[1].text)\n",
    "    ratings.append(data[2].text)\n",
    "    \n",
    "batsmen_df = pd.DataFrame({\n",
    "    'Player': players,\n",
    "    'Team': teams,\n",
    "    'Rating': ratings  \n",
    "})\n",
    "\n",
    "# Scrape ODI Bowlers Rankings\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for row in soup.find('table', class_='table').find_all('tr')[1:11]:\n",
    "    data = row.find_all('td')\n",
    "    players.append(data[0].text)\n",
    "    teams.append(data[1].text)\n",
    "    ratings.append(data[2].text)\n",
    "    \n",
    "bowlers_df = pd.DataFrame({\n",
    "    'Player': players,\n",
    "    'Team': teams, \n",
    "    'Rating': ratings\n",
    "})\n",
    "\n",
    "print(teams_df)\n",
    "print(batsmen_df) \n",
    "print(bowlers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5eb5fa",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and extract data\n",
    "def scrape_and_extract_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a dataframe from the extracted data\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Function to scrape and parse the ODI rankings\n",
    "def scrape_odi_rankings(url, ranking_type):\n",
    "    soup = scrape_and_extract_data(url)\n",
    "    if soup:\n",
    "        table = soup.find('table', {'class': 'table rankings-table'})\n",
    "        data = []\n",
    "        columns = [\"Position\", \"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "\n",
    "        for row in table.find_all('tr')[1:11]:\n",
    "            row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            data.append(row_data)\n",
    "\n",
    "        return create_dataframe(data, columns)\n",
    "\n",
    "# Function to scrape and parse the ODI player rankings\n",
    "def scrape_player_rankings(url, ranking_type):\n",
    "    soup = scrape_and_extract_data(url)\n",
    "    if soup:\n",
    "        table = soup.find('table', {'class': 'table rankings-table'})\n",
    "        data = []\n",
    "        columns = [\"Position\", \"Player\", \"Team\", \"Rating\"]\n",
    "\n",
    "        for row in table.find_all('tr')[1:11]:\n",
    "            row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            data.append(row_data)\n",
    "\n",
    "        return create_dataframe(data, columns)\n",
    "\n",
    "# Main program\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "odi_batting_players_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "odi_teams_df = scrape_odi_rankings(odi_teams_url, \"ODI Teams\")\n",
    "odi_batting_players_df = scrape_player_rankings(odi_batting_players_url, \"ODI Batting Players\")\n",
    "odi_allrounders_df = scrape_player_rankings(odi_allrounders_url, \"ODI All-rounders\")\n",
    "\n",
    "# Print the dataframes\n",
    "print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(odi_batting_players_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "print(odi_allrounders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585631a5",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38855559",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m news_links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate through the news articles\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnews_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCard-title\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     24\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m     time \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_next(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the CNBC World news page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the div containing the news articles\n",
    "news_container = soup.find(\"div\", class_=\"Card-group\")\n",
    "\n",
    "# Create empty lists to store data\n",
    "headlines = []\n",
    "times = []\n",
    "news_links = []\n",
    "\n",
    "# Iterate through the news articles\n",
    "for article in news_container.find_all(\"div\", class_=\"Card-title\"):\n",
    "    headline = article.text.strip()\n",
    "    time = article.find_next(\"time\").text.strip()\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    news_links.append(link)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Headline\": headlines,\n",
    "    \"Time\": times,\n",
    "    \"News Link\": news_links,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a526a6",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c6b8796",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m paper_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Iterate through the articles\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpod-list-item\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     25\u001b[0m     title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m     author \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpod-list-authors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the div containing the most downloaded articles\n",
    "articles_container = soup.find(\"div\", class_=\"pod-list\")\n",
    "\n",
    "# Create empty lists to store data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "paper_urls = []\n",
    "\n",
    "# Iterate through the articles\n",
    "for article in articles_container.find_all(\"div\", class_=\"pod-list-item\"):\n",
    "    title = article.find(\"h2\").text.strip()\n",
    "    author = article.find(\"div\", class_=\"pod-list-authors\").text.strip()\n",
    "    date = article.find(\"div\", class_=\"pod-list-date\").text.strip()\n",
    "    paper_url = article.find(\"a\", class_=\"pod-list-link\")[\"href\"]\n",
    "\n",
    "    titles.append(title)\n",
    "    authors.append(author)\n",
    "    dates.append(date)\n",
    "    paper_urls.append(paper_url)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Paper Title\": titles,\n",
    "    \"Authors\": authors,\n",
    "    \"Published Date\": dates,\n",
    "    \"Paper URL\": paper_urls,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965dda5f",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "856ebef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details:\n",
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and extract restaurant data\n",
    "def scrape_and_extract_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a DataFrame from the extracted data\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Function to scrape restaurant details\n",
    "def scrape_restaurant_details(url):\n",
    "    soup = scrape_and_extract_data(url)\n",
    "    if soup:\n",
    "        data = []\n",
    "        columns = [\"Restaurant Name\", \"Cuisine\", \"Location\", \"Ratings\", \"Image URL\"]\n",
    "\n",
    "        restaurants = soup.find_all(\"li\", class_=\"restnt-card restaurant\")\n",
    "        for restaurant in restaurants:\n",
    "            name = restaurant.find(\"div\", class_=\"restnt-info\").find(\"h2\").text.strip()\n",
    "            cuisine = restaurant.find(\"div\", class_=\"restnt-info\").find(\"p\", class_=\"double-line-ellipsis\").text.strip()\n",
    "            location = restaurant.find(\"div\", class_=\"restnt-info\").find(\"p\", class_=\"restnt-loc ellipsis\").text.strip()\n",
    "            ratings = restaurant.find(\"div\", class_=\"rating rating-outer\").text.strip()\n",
    "            image_url = restaurant.find(\"img\")[\"src\"]\n",
    "\n",
    "            data.append([name, cuisine, location, ratings, image_url])\n",
    "\n",
    "        return create_dataframe(data, columns)\n",
    "\n",
    "# Main program\n",
    "dineout_url = \"https://www.dineout.co.in/bangalore-restaurants\"\n",
    "restaurant_df = scrape_restaurant_details(dineout_url)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Restaurant Details:\")\n",
    "print(restaurant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32478c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
