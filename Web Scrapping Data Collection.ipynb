{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed02ad7",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec41392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\anaconda\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbdec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL and search parameters\n",
    "url = \"https://www.shine.com/job-search/data-analyst-jobs-in-bangalore\"\n",
    "# You may need to inspect the website to get the correct input field names\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "# Parameters for the first 10 jobs\n",
    "num_jobs = 10\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find job listings on the page\n",
    "    job_listings = soup.find_all(\"div\", class_=\"srm-link\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    # Extract data for the first 10 job listings\n",
    "    for job in job_listings[:num_jobs]:\n",
    "        job_title = job.find(\"a\", class_=\"job-title\").text.strip()\n",
    "        job_location = job.find(\"div\", class_=\"job-location\").text.strip()\n",
    "        company_name = job.find(\"span\", class_=\"job-title\").text.strip()\n",
    "        exp = job.find(\"div\", class_=\"job-eligibility\").text.strip()\n",
    "\n",
    "        job_titles.append(job_title)\n",
    "        job_locations.append(job_location)\n",
    "        company_names.append(company_name)\n",
    "        experience_required.append(exp)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names,\n",
    "        \"Experience Required\": experience_required,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2f00c",
   "metadata": {},
   "source": [
    "Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba3ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL and search parameters\n",
    "url = \"https://www.shine.com/job-search/data-scientist-jobs-in-bangalore\"\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "num_jobs = 10\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find job listings on the page\n",
    "    job_listings = soup.find_all(\"div\", class_=\"srm-link\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "\n",
    "    # Extract data for the first 10 job listings\n",
    "    for job in job_listings[:num_jobs]:\n",
    "        job_title = job.find(\"a\", class_=\"job-title\").text.strip()\n",
    "        job_location = job.find(\"div\", class_=\"job-location\").text.strip()\n",
    "        company_name = job.find(\"span\", class_=\"job-title\").text.strip()\n",
    "\n",
    "        job_titles.append(job_title)\n",
    "        job_locations.append(job_location)\n",
    "        company_names.append(company_name)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed988d5",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c9424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.shine.com/job-search/data-scientist-jobs-in-delhi-ncr\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Apply filters for location and salary range\n",
    "    # You may need to inspect the website to get the correct input field names\n",
    "    location_filter = soup.find(\"input\", {\"value\": \"Delhi/NCR\"})\n",
    "    if location_filter:\n",
    "        location_filter[\"checked\"] = \"checked\"\n",
    "\n",
    "    salary_filter = soup.find(\"input\", {\"value\": \"3-6 Lakhs\"})\n",
    "    if salary_filter:\n",
    "        salary_filter[\"checked\"] = \"checked\"\n",
    "\n",
    "    # Click the search button to apply the filters (you may need to inspect the website to find the button's class or ID)\n",
    "    # Add code here to click the search button (using a library like Selenium or other automation tools)\n",
    "\n",
    "    # Find job listings on the filtered page\n",
    "    job_listings = soup.find_all(\"div\", class_=\"srm-link\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    # Extract data for the first 10 job listings\n",
    "    for job in job_listings[:10]:\n",
    "        job_title = job.find(\"a\", class_=\"job-title\").text.strip()\n",
    "        job_location = job.find(\"div\", class_=\"job-location\").text.strip()\n",
    "        company_name = job.find(\"span\", class_=\"job-title\").text.strip()\n",
    "        exp = job.find(\"div\", class_=\"job-eligibility\").text.strip()\n",
    "\n",
    "        job_titles.append(job_title)\n",
    "        job_locations.append(job_location)\n",
    "        company_names.append(company_name)\n",
    "        experience_required.append(exp)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names,\n",
    "        \"Experience Required\": experience_required,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6ad69",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94487d8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2778380347.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 46\u001b[1;36m\u001b[0m\n\u001b[1;33m    next_button = base_url + soup.find(\"a\", class=\"_1LKTO3\")[\"href\"]\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Initialize a counter for the number of sunglasses scraped\n",
    "sunglasses_count = 0\n",
    "\n",
    "# Set the URL for the Flipkart sunglasses search\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "search_query = \"sunglasses\"\n",
    "url = f\"{base_url}/search?q={search_query}\"\n",
    "\n",
    "# Function to scrape and extract data from a Flipkart page\n",
    "def scrape_flipkart_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        for product in products:\n",
    "            brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "            description = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "            price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "\n",
    "            brands.append(brand)\n",
    "            descriptions.append(description)\n",
    "            prices.append(price)\n",
    "\n",
    "            global sunglasses_count\n",
    "            sunglasses_count += 1\n",
    "\n",
    "            if sunglasses_count >= 100:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Scrape the data from Flipkart's search results pages\n",
    "while True:\n",
    "    if scrape_flipkart_page(url):\n",
    "        break\n",
    "    next_button = base_url + soup.find(\"a\", class=\"_1LKTO3\")[\"href\"]\n",
    "    url = next_button\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": descriptions,\n",
    "    \"Price\": prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90dfd0",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ee1df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (244705738.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    review_summary = review_box.find(\"p\", class=\"_2-N8zT\").text\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped review data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Set the URL for the iPhone 11 reviews\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Function to scrape and extract review data from a Flipkart page\n",
    "def scrape_flipkart_reviews(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        review_boxes = soup.find_all(\"div\", class_=\"_27M-vq\")\n",
    "\n",
    "        for review_box in review_boxes:\n",
    "            rating = review_box.find(\"div\", class_=\"_3LWZlK\").text\n",
    "            review_summary = review_box.find(\"p\", class=\"_2-N8zT\").text\n",
    "            full_review = review_box.find(\"div\", class_=\"t-ZTKy\").text\n",
    "\n",
    "            ratings.append(rating)\n",
    "            review_summaries.append(review_summary)\n",
    "            full_reviews.append(full_review)\n",
    "\n",
    "            if len(ratings) >= 100:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Scrape the data from Flipkart's review pages\n",
    "while True:\n",
    "    if scrape_flipkart_reviews(url):\n",
    "        break\n",
    "    next_button = soup.find(\"a\", class_=\"_1LKTO3\")\n",
    "    if not next_button:\n",
    "        break\n",
    "    url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
    "\n",
    "# Create a DataFrame with the scraped review data\n",
    "data = {\n",
    "    \"Rating\": ratings,\n",
    "    \"Review Summary\": review_summaries,\n",
    "    \"Full Review\": full_reviews\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff38010",
   "metadata": {},
   "source": [
    "Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb39b452",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Scrape the data from Flipkart's search results pages\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscrape_flipkart_sneakers\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     next_button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_1LKTO3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mscrape_flipkart_sneakers\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     18\u001b[0m products \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_1AtVbE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products:\n\u001b[1;32m---> 21\u001b[0m     brand \u001b[38;5;241m=\u001b[39m \u001b[43mproduct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_2WkVRV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[0;32m     22\u001b[0m     description \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     23\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped sneaker data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Set the URL for the Flipkart sneaker search\n",
    "url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "\n",
    "# Function to scrape and extract data from a Flipkart page\n",
    "def scrape_flipkart_sneakers(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        for product in products:\n",
    "            brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "            description = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "            price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "\n",
    "            brands.append(brand)\n",
    "            descriptions.append(description)\n",
    "            prices.append(price)\n",
    "\n",
    "            if len(brands) >= 100:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Scrape the data from Flipkart's search results pages\n",
    "while True:\n",
    "    if scrape_flipkart_sneakers(url):\n",
    "        break\n",
    "    next_button = soup.find(\"a\", class_=\"_1LKTO3\")\n",
    "    if not next_button:\n",
    "        break\n",
    "    url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
    "\n",
    "# Create a DataFrame with the scraped sneaker data\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": descriptions,\n",
    "    \"Price\": prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29765efc",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c8a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Ratings, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped laptop data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Set the URL for the Amazon.in laptop search with filters\n",
    "url = \"https://www.amazon.in/s?k=Laptop&i=computers&field-enc-merchantbin=ATVPDKIKX0DER\"\n",
    "# Set the CPU Type filter for \"Intel Core i7\"\n",
    "url += \"&rh=n%3A1375424031%2Cp_n_feature_five_browse-bin%3A7326076031\"\n",
    "\n",
    "# Function to scrape and extract data from an Amazon page\n",
    "def scrape_amazon_laptops(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "        for product in products[:10]:\n",
    "            title = product.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "            rating = product.find(\"span\", class_=\"a-icon-alt\")\n",
    "            price = product.find(\"span\", class_=\"a-price-whole\")\n",
    "\n",
    "            # Some products may not have ratings or prices, so handle these cases\n",
    "            rating = rating.text if rating else \"N/A\"\n",
    "            price = price.text if price else \"N/A\"\n",
    "\n",
    "            titles.append(title)\n",
    "            ratings.append(rating)\n",
    "            prices.append(price)\n",
    "\n",
    "    return len(titles) >= 10\n",
    "\n",
    "# Scrape the data from Amazon's search results page\n",
    "scrape_amazon_laptops(url)\n",
    "\n",
    "# Create a DataFrame with the scraped laptop data\n",
    "data = {\n",
    "    \"Title\": titles,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Price\": prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bfec8",
   "metadata": {},
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0284c027",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3428164154.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    quote = quote_div.find(\"a\", class=\"title\")\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped quote data\n",
    "quotes = []\n",
    "authors = []\n",
    "types_of_quotes = []\n",
    "\n",
    "# Set the URL for AzQuotes\n",
    "url = \"https://www.azquotes.com/\"\n",
    "\n",
    "# Function to scrape and extract data from the AzQuotes page\n",
    "def scrape_azquotes(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Click on the \"Top Quotes\" link\n",
    "        top_quotes_link = soup.find(\"a\", text=\"Top Quotes\")\n",
    "        if top_quotes_link:\n",
    "            top_quotes_url = url + top_quotes_link[\"href\"]\n",
    "            response = requests.get(top_quotes_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                quote_divs = soup.find_all(\"div\", class_=\"clearfix\")\n",
    "                for quote_div in quote_divs:\n",
    "                    quote = quote_div.find(\"a\", class=\"title\")\n",
    "                    author = quote_div.find(\"a\", class=\"author\")\n",
    "                    type_of_quote = quote_div.find(\"a\", class=\"tag\")\n",
    "                    if quote and author:\n",
    "                        quotes.append(quote.text.strip())\n",
    "                        authors.append(author.text.strip())\n",
    "                        types_of_quotes.append(type_of_quote.text.strip() if type_of_quote else \"N/A\")\n",
    "\n",
    "# Scrape the data from AzQuotes\n",
    "scrape_azquotes(url)\n",
    "\n",
    "# Create a DataFrame with the scraped quote data\n",
    "data = {\n",
    "    \"Quote\": quotes,\n",
    "    \"Author\": authors,\n",
    "    \"Type Of Quotes\": types_of_quotes\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4608f8",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13ff799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Born-Dead, Term of Office, Remarks]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped Prime Minister data\n",
    "names = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Set the URL for Jagran Josh\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "\n",
    "# Function to scrape and extract data from the Jagran Josh page\n",
    "def scrape_jagranjosh(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Click on the \"GK\" option\n",
    "        gk_link = soup.find(\"a\", text=\"GK\")\n",
    "        if gk_link:\n",
    "            gk_url = url + gk_link[\"href\"]\n",
    "            response = requests.get(gk_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                # Click on the \"List of all Prime Ministers of India\" link\n",
    "                pm_link = soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "                if pm_link:\n",
    "                    pm_url = pm_link[\"href\"]\n",
    "                    response = requests.get(pm_url)\n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                        pm_table = soup.find(\"table\", class_=\"mktb\")\n",
    "                        rows = pm_table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "                        for row in rows:\n",
    "                            columns = row.find_all(\"td\")\n",
    "                            name = columns[0].text.strip()\n",
    "                            born_dead_info = columns[1].text.strip()\n",
    "                            term_info = columns[2].text.strip()\n",
    "                            remark = columns[3].text.strip()\n",
    "\n",
    "                            names.append(name)\n",
    "                            born_dead.append(born_dead_info)\n",
    "                            term_of_office.append(term_info)\n",
    "                            remarks.append(remark)\n",
    "\n",
    "# Scrape the data from Jagran Josh\n",
    "scrape_jagranjosh(url)\n",
    "\n",
    "# Create a DataFrame with the scraped Prime Minister data\n",
    "data = {\n",
    "    \"Name\": names,\n",
    "    \"Born-Dead\": born_dead,\n",
    "    \"Term of Office\": term_of_office,\n",
    "    \"Remarks\": remarks\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e8a0c",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "786fe79c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m                     car_prices\u001b[38;5;241m.\u001b[39mappend(car_price)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Scrape the data from Motor1.com\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mscrape_motor1\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with the scraped car data\u001b[39;00m\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCar Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: car_names,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m: car_prices\n\u001b[0;32m     43\u001b[0m }\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mscrape_motor1\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Type \"50 most expensive cars\" in the search bar and click the search button\u001b[39;00m\n\u001b[0;32m     18\u001b[0m search_bar \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm1-autosuggest-input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m search_bar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50 most expensive cars\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m search_button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm1-autosuggest-button\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m search_button\u001b[38;5;241m.\u001b[39mclick()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped car data\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# Set the URL for Motor1.com\n",
    "url = \"https://www.motor1.com/\"\n",
    "\n",
    "# Function to scrape and extract data from the Motor1.com page\n",
    "def scrape_motor1(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Type \"50 most expensive cars\" in the search bar and click the search button\n",
    "        search_bar = soup.find(\"input\", class_=\"m1-autosuggest-input\")\n",
    "        search_bar[\"value\"] = \"50 most expensive cars\"\n",
    "        search_button = soup.find(\"button\", class_=\"m1-autosuggest-button\")\n",
    "        search_button.click()\n",
    "        # Click on \"50 Most Expensive Cars In The World\" link\n",
    "        expensive_cars_link = soup.find(\"a\", text=\"50 Most Expensive Cars In The World\")\n",
    "        if expensive_cars_link:\n",
    "            expensive_cars_url = url + expensive_cars_link[\"href\"]\n",
    "            response = requests.get(expensive_cars_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                car_list = soup.find_all(\"div\", class_=\"card-text\")\n",
    "                for car in car_list:\n",
    "                    car_name = car.find(\"h2\").text.strip()\n",
    "                    car_price = car.find(\"span\", class_=\"m1-car-price\").text.strip()\n",
    "                    car_names.append(car_name)\n",
    "                    car_prices.append(car_price)\n",
    "\n",
    "# Scrape the data from Motor1.com\n",
    "scrape_motor1(url)\n",
    "\n",
    "# Create a DataFrame with the scraped car data\n",
    "data = {\n",
    "    \"Car Name\": car_names,\n",
    "    \"Price\": car_prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdebc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
