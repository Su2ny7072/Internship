{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cd8926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\anaconda\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86132295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee74e35",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c325b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3baf44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1., Name: \"Baby Shark Dance\"[6], Artist: Pinkfong Baby Shark - Kids' Songs & Stories, Upload Date: June 17, 2016, Views: 13.65\n",
      "Rank: 2., Name: \"Despacito\"[9], Artist: Luis Fonsi, Upload Date: January 12, 2017, Views: 8.32\n",
      "Rank: 3., Name: \"Johny Johny Yes Papa\"[17], Artist: LooLoo Kids - Nursery Rhymes and Children's Songs, Upload Date: October 8, 2016, Views: 6.84\n",
      "Rank: 4., Name: \"Bath Song\"[18], Artist: Cocomelon - Nursery Rhymes, Upload Date: May 2, 2018, Views: 6.50\n",
      "Rank: 5., Name: \"Shape of You\"[19], Artist: Ed Sheeran, Upload Date: January 30, 2017, Views: 6.14\n",
      "Rank: 6., Name: \"See You Again\"[22], Artist: Wiz Khalifa, Upload Date: April 6, 2015, Views: 6.09\n",
      "Rank: 7., Name: \"Wheels on the Bus\"[27], Artist: Cocomelon - Nursery Rhymes, Upload Date: May 24, 2018, Views: 5.71\n",
      "Rank: 8., Name: \"Phonics Song with Two Words\"[28], Artist: ChuChu TV Nursery Rhymes & Kids Songs, Upload Date: March 6, 2014, Views: 5.57\n",
      "Rank: 9., Name: \"Uptown Funk\"[29], Artist: Mark Ronson, Upload Date: November 19, 2014, Views: 5.09\n",
      "Rank: 10., Name: \"Learning Colors – Colorful Eggs on a Farm\"[30], Artist: Miroshka TV, Upload Date: February 27, 2018, Views: 5.01\n",
      "Rank: 11., Name: \"Gangnam Style\"[31], Artist: officialpsy, Upload Date: July 15, 2012, Views: 4.96\n",
      "Rank: 12., Name: \"Masha and the Bear – Recipe for Disaster\"[36], Artist: Get Movies, Upload Date: January 31, 2012, Views: 4.57\n",
      "Rank: 13., Name: \"Dame Tu Cosita\"[37], Artist: Ultra Records, Upload Date: April 5, 2018, Views: 4.48\n",
      "Rank: 14., Name: \"Axel F\"[38], Artist: Crazy Frog, Upload Date: June 16, 2009, Views: 4.16\n",
      "Rank: 15., Name: \"Sugar\"[39], Artist: Maroon 5, Upload Date: January 14, 2015, Views: 3.97\n",
      "Rank: 16., Name: \"Counting Stars\"[40], Artist: OneRepublic, Upload Date: May 31, 2013, Views: 3.92\n",
      "Rank: 17., Name: \"Roar\"[41], Artist: Katy Perry, Upload Date: September 5, 2013, Views: 3.91\n",
      "Rank: 18., Name: \"Baa Baa Black Sheep\"[42], Artist: Cocomelon - Nursery Rhymes, Upload Date: June 25, 2018, Views: 3.84\n",
      "Rank: 19., Name: \"Waka Waka (This Time for Africa)\"[43], Artist: Shakira, Upload Date: June 4, 2010, Views: 3.78\n",
      "Rank: 20., Name: \"Lakdi Ki Kathi\"[44], Artist: Jingle Toons, Upload Date: June 14, 2018, Views: 3.76\n",
      "Rank: 21., Name: \"Sorry\"[45], Artist: Justin Bieber, Upload Date: October 22, 2015, Views: 3.74\n",
      "Rank: 22., Name: \"Thinking Out Loud\"[46], Artist: Ed Sheeran, Upload Date: October 7, 2014, Views: 3.69\n",
      "Rank: 23., Name: \"Humpty the train on a fruits ride\"[47], Artist: Kiddiestv Hindi - Nursery Rhymes & Kids Songs, Upload Date: January 26, 2018, Views: 3.63\n",
      "Rank: 24., Name: \"Dark Horse\"[48], Artist: Katy Perry, Upload Date: February 20, 2014, Views: 3.63\n",
      "Rank: 25., Name: \"Perfect\"[49], Artist: Ed Sheeran, Upload Date: November 9, 2017, Views: 3.60\n",
      "Rank: 26., Name: \"Let Her Go\"[50], Artist: Passenger, Upload Date: July 25, 2012, Views: 3.56\n",
      "Rank: 27., Name: \"Faded\"[51], Artist: Alan Walker, Upload Date: December 3, 2015, Views: 3.55\n",
      "Rank: 28., Name: \"Shree Hanuman Chalisa\"[52], Artist: T-Series Bhakti Sagar, Upload Date: May 10, 2011, Views: 3.54\n",
      "Rank: 29., Name: \"Girls Like You\"[53], Artist: Maroon 5, Upload Date: May 31, 2018, Views: 3.52\n",
      "Rank: 30., Name: \"Lean On\"[54], Artist: Major Lazer Official, Upload Date: March 22, 2015, Views: 3.50\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming the structure of the table is consistent, you can extract the details like this:\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m title \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m artist \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the most-viewed videos\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Extract the details from the table\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Assuming the structure of the table is consistent, you can extract the details like this:\n",
    "    rank = columns[0].text.strip()\n",
    "    title = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip()\n",
    "\n",
    "    print(f\"Rank: {rank}, Name: {title}, Artist: {artist}, Upload Date: {upload_date}, Views: {views}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06499a5",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c0da424",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m fixtures \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem-348\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mfixtures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      7\u001b[0m cricket_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "fixtures = soup.find('div', {'class':'item-348'})\n",
    "rows = fixtures.find_all('tr')[1:]\n",
    "\n",
    "cricket_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    series = cols[1].text.strip() \n",
    "    place = cols[2].text.strip()\n",
    "    date = cols[3].text.strip()\n",
    "    time = cols[4].text.strip()\n",
    "    \n",
    "    cricket_data.append({\n",
    "        'Series':series,\n",
    "        'Place':place, \n",
    "        'Date':date,\n",
    "        'Time':time\n",
    "    })\n",
    "    \n",
    "cricket_df = pd.DataFrame(cricket_data)\n",
    "print(cricket_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32432527",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e5448f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m gdp \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-responsive\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mgdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      7\u001b[0m gdp_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "gdp = soup.find('div', {'class':'table-responsive'})\n",
    "rows = gdp.find_all('tr')[1:]\n",
    "\n",
    "gdp_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    rank = cols[0].text.strip() \n",
    "    state = cols[1].text.strip()\n",
    "    gsdp18 = cols[2].text.strip()\n",
    "    gsdp19 = cols[3].text.strip()\n",
    "    share18 = cols[4].text.strip()   \n",
    "    gdp_bn = cols[5].text.strip()\n",
    "    \n",
    "    gdp_data.append({\n",
    "        'Rank':rank,\n",
    "        'State':state,\n",
    "        'GSDP 18-19':gsdp18,\n",
    "        'GSDP 19-20':gsdp19,\n",
    "        'Share 18-19':share18,\n",
    "        'GDP($ Billion)':gdp_bn\n",
    "    })\n",
    "    \n",
    "gdp_df = pd.DataFrame(gdp_data) \n",
    "print(gdp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8aa25",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ce9412",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m repos \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication-main\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mrepos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox-row\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      7\u001b[0m github_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "repos = soup.find('div', {'class':'application-main'})\n",
    "rows = repos.find_all('div', {'class':'Box-row'})\n",
    "\n",
    "github_data = []\n",
    "for row in rows:\n",
    "    title = row.find('h1').text.strip()\n",
    "    desc = row.find('p', {'class':'col-9'}).text.strip()\n",
    "    \n",
    "    contributors = row.find('svg',{'class':'octicon octicon-organization'})\n",
    "    contributors = contributors.next_sibling.strip() if contributors else ''\n",
    "    \n",
    "    language = row.find('span', {'itemprop':'programmingLanguage'})\n",
    "    language = language.text if language else ''\n",
    "    \n",
    "    github_data.append({\n",
    "        'Title':title,\n",
    "        'Description':desc,\n",
    "        'Contributors':contributors,\n",
    "        'Language':language\n",
    "    })\n",
    "    \n",
    "github_df = pd.DataFrame(github_data)    \n",
    "print(github_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbade1b",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fcc5f46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m songs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchart-results-list\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[43msongs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-chart-results-list-item\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      7\u001b[0m billboard_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "songs = soup.find('div', {'class':'chart-results-list'})\n",
    "items = songs.find_all('div', {'class':'o-chart-results-list-item'})\n",
    "\n",
    "billboard_data = []\n",
    "for item in items:\n",
    "    title = item.find('h3').text.strip()\n",
    "    artist = item.find('span').text.strip()\n",
    "    \n",
    "    meta = item.find('div', {'class':'o-chart-results-list-item__meta'}).find_all('span')\n",
    "    last_week = meta[0].text.strip()\n",
    "    peak_rank = meta[1].text.strip()\n",
    "    weeks_on_board = meta[2].text.strip()\n",
    "    \n",
    "    billboard_data.append({\n",
    "        'Song':title,\n",
    "        'Artist':artist,\n",
    "        'Last Week':last_week,  \n",
    "        'Peak Rank':peak_rank,\n",
    "        'Weeks on Board':weeks_on_board\n",
    "    })\n",
    "    \n",
    "billboard_df = pd.DataFrame(billboard_data)\n",
    "print(billboard_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687eb4dd",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0597c771",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m cols \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m name \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 13\u001b[0m author \u001b[38;5;241m=\u001b[39m \u001b[43mcols\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     14\u001b[0m volumes \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m–\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     15\u001b[0m publisher \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "books_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    \n",
    "    name = cols[0].text.strip()\n",
    "    author = cols[1].text.strip()\n",
    "    volumes = cols[2].text.strip().replace('\\xa0–\\xa0', '-') \n",
    "    publisher = cols[3].text.strip()\n",
    "    genre = cols[4].text.strip()\n",
    "    \n",
    "    books_data.append({\n",
    "        'Book':name,\n",
    "        'Author':author, \n",
    "        'Volumes Sold':volumes,\n",
    "        'Publisher':publisher,\n",
    "        'Genre':genre\n",
    "    })\n",
    "    \n",
    "books_df = pd.DataFrame(books_data)\n",
    "print(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f73538",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b52e8c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m series \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlister list detail sub-list\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlister-item mode-detail\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      7\u001b[0m imdb_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "series = soup.find('div', {'class':'lister list detail sub-list'})\n",
    "items = series.find_all('div', {'class':'lister-item mode-detail'})\n",
    "\n",
    "imdb_data = []\n",
    "for item in items:\n",
    "    title = item.h3.a.text.strip()  \n",
    "    year_span = item.h3.find('span').text.strip()\n",
    "    genre = item.find('span', {'class':'genre'}).text.strip().replace('\\n', '')\n",
    "    runtime = item.find('span', {'class':'runtime'}).text.strip()  \n",
    "    rating = item.find('div', {'class':'ipl-rating-star__rating'}).text.strip()\n",
    "    votes = item.find('span', {'name':'nv'}).text.strip()\n",
    "    \n",
    "    imdb_data.append({\n",
    "        'Name':title,\n",
    "        'Year Span':year_span,\n",
    "        'Genre':genre,\n",
    "        'Runtime':runtime,\n",
    "        'Ratings':rating,\n",
    "        'Votes':votes        \n",
    "    })\n",
    "    \n",
    "imdb_df = pd.DataFrame(imdb_data)\n",
    "print(imdb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7f5d9",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39fdec48",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcellpadding\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      7\u001b[0m uci_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'cellpadding':'3'})\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "uci_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    \n",
    "    name = cols[0].text.strip()\n",
    "    datatype = cols[1].text.strip()  \n",
    "    task = cols[2].text.strip()\n",
    "    attribute_type = cols[3].text.strip()\n",
    "    instances = cols[4].text.strip()  \n",
    "    attributes = cols[5].text.strip()\n",
    "    year = cols[6].text.strip()\n",
    "    \n",
    "    uci_data.append({\n",
    "        'Dataset':name,\n",
    "        'Data Type':datatype,\n",
    "        'Task':task,\n",
    "        'Attribute Type':attribute_type,\n",
    "        'No. of Instances':instances,\n",
    "        'No. of Attributes':attributes,\n",
    "        'Year':year\n",
    "    })\n",
    "    \n",
    "uci_df = pd.DataFrame(uci_data) \n",
    "print(uci_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a57509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
